2025-06-22 01:57:02,787 [INFO] Using device: cpu
2025-06-22 01:57:02,788 [INFO] 开始创建数据加载器...
2025-06-22 01:57:02,789 [INFO] Successfully loaded dataset ENZYMES from ./data, total graphs: 600
2025-06-22 01:57:02,789 [INFO] Class allocation analysis:
2025-06-22 01:57:02,789 [INFO]   Source classes: [0, 1, 2, 3]
2025-06-22 01:57:02,789 [INFO]   Target classes: [0, 1, 2, 3, 4, 5]
2025-06-22 01:57:02,789 [INFO]   Overlap classes (to be split): [0, 1, 2, 3]
2025-06-22 01:57:02,789 [INFO]   Target novel classes (unknown): [4, 5]
2025-06-22 01:57:02,804 [INFO]   Class 0: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,804 [INFO]   Class 1: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,804 [INFO]   Class 2: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,804 [INFO]   Class 3: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,804 [INFO]   Class 4 (novel/unknown): 100 samples -> Target
2025-06-22 01:57:02,804 [INFO]   Class 5 (novel/unknown): 100 samples -> Target
2025-06-22 01:57:02,804 [INFO] Final allocation:
2025-06-22 01:57:02,804 [INFO]   Source domain: 280 samples
2025-06-22 01:57:02,804 [INFO]   Target domain: 320 samples
2025-06-22 01:57:02,806 [INFO] Source domain setup:
2025-06-22 01:57:02,806 [INFO]   Original classes: [0, 1, 2, 3]
2025-06-22 01:57:02,806 [INFO]   Overlap classes used: [0, 1, 2, 3]
2025-06-22 01:57:02,806 [INFO]   Label mapping: {0: 0, 1: 1, 2: 2, 3: 3}
2025-06-22 01:57:02,806 [INFO]   Total data after allocation: 280
2025-06-22 01:57:02,806 [INFO]   Data split - Train: 224, Val: 56
2025-06-22 01:57:02,806 [INFO]   Train label distribution: {0: 56, 1: 56, 3: 56, 2: 56}
2025-06-22 01:57:02,806 [INFO]   Val label distribution: {1: 14, 2: 14, 0: 14, 3: 14}
2025-06-22 01:57:02,807 [INFO] Successfully loaded dataset ENZYMES from ./data, total graphs: 600
2025-06-22 01:57:02,807 [INFO] Class allocation analysis:
2025-06-22 01:57:02,807 [INFO]   Source classes: [0, 1, 2, 3]
2025-06-22 01:57:02,807 [INFO]   Target classes: [0, 1, 2, 3, 4, 5]
2025-06-22 01:57:02,807 [INFO]   Overlap classes (to be split): [0, 1, 2, 3]
2025-06-22 01:57:02,807 [INFO]   Target novel classes (unknown): [4, 5]
2025-06-22 01:57:02,821 [INFO]   Class 0: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,822 [INFO]   Class 1: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,822 [INFO]   Class 2: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,822 [INFO]   Class 3: 100 total -> Source: 70, Target: 30
2025-06-22 01:57:02,822 [INFO]   Class 4 (novel/unknown): 100 samples -> Target
2025-06-22 01:57:02,822 [INFO]   Class 5 (novel/unknown): 100 samples -> Target
2025-06-22 01:57:02,822 [INFO] Final allocation:
2025-06-22 01:57:02,822 [INFO]   Source domain: 280 samples
2025-06-22 01:57:02,822 [INFO]   Target domain: 320 samples
2025-06-22 01:57:02,824 [INFO] Target domain setup:
2025-06-22 01:57:02,824 [INFO]   Original classes: [0, 1, 2, 3, 4, 5]
2025-06-22 01:57:02,824 [INFO]   Overlap classes: [0, 1, 2, 3]
2025-06-22 01:57:02,824 [INFO]   Novel classes: [4, 5] -> mapped to unknown label 4
2025-06-22 01:57:02,824 [INFO]   Label mapping: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 4}
2025-06-22 01:57:02,824 [INFO]   Total data after allocation: 320
2025-06-22 01:57:02,824 [INFO]   Data split - Train: 256, Val: 64
2025-06-22 01:57:02,824 [INFO]   Total count: 320 (known: 120, unknown: 200)
2025-06-22 01:57:02,824 [INFO]   Total label distribution: {4: 200, 3: 30, 0: 30, 1: 30, 2: 30}
2025-06-22 01:57:02,824 [INFO]   Train label distribution: {4: 160, 0: 24, 2: 24, 3: 24, 1: 24}
2025-06-22 01:57:02,824 [INFO]   Val label distribution: {1: 6, 4: 40, 3: 6, 0: 6, 2: 6}
2025-06-22 01:57:02,824 [INFO] 数据加载器创建完成!
2025-06-22 01:57:02,824 [INFO] 源域类别数: 4 (重叠类别)
2025-06-22 01:57:02,824 [INFO] 目标域类别数: 5 (4个重叠类别 + 1个未知类别)
2025-06-22 01:57:02,824 [INFO] 重叠类别: [0, 1, 2, 3]
2025-06-22 01:57:02,824 [INFO] 目标域新类别: [4, 5]
2025-06-22 01:57:02,824 [INFO] Data allocation verification:
2025-06-22 01:57:02,824 [INFO]   Source data samples: 280
2025-06-22 01:57:02,824 [INFO]   Target data samples: 320
2025-06-22 01:57:02,824 [INFO]   Overlapping samples: 0
2025-06-22 01:57:02,824 [INFO]   ✓ Data allocation is correct - no overlapping samples
2025-06-22 01:57:02,826 [INFO] Model initialized with 3684 parameters
2025-06-22 01:57:02,826 [INFO] Previous best validation accuracy read from record.txt: 0.6250
2025-06-22 01:57:02,826 [INFO] Starting training...
2025-06-22 01:57:02,840 [INFO] Epoch 001 | Train Loss: 6.6322 | Val Loss: 2.1275 | Val Acc: 0.3393
2025-06-22 01:57:03,303 [INFO] New best model at epoch 1 (Val Acc: 0.3393)
2025-06-22 01:57:03,318 [INFO] Epoch 002 | Train Loss: 5.3837 | Val Loss: 1.8890 | Val Acc: 0.3571
2025-06-22 01:57:03,320 [INFO] New best model at epoch 2 (Val Acc: 0.3571)
2025-06-22 01:57:03,333 [INFO] Epoch 003 | Train Loss: 5.2752 | Val Loss: 1.7972 | Val Acc: 0.4286
2025-06-22 01:57:03,336 [INFO] New best model at epoch 3 (Val Acc: 0.4286)
2025-06-22 01:57:03,350 [INFO] Epoch 004 | Train Loss: 4.2125 | Val Loss: 1.6931 | Val Acc: 0.3571
2025-06-22 01:57:03,365 [INFO] Epoch 005 | Train Loss: 4.2219 | Val Loss: 1.5737 | Val Acc: 0.3929
2025-06-22 01:57:03,379 [INFO] Epoch 006 | Train Loss: 3.6292 | Val Loss: 1.5741 | Val Acc: 0.4107
2025-06-22 01:57:03,394 [INFO] Epoch 007 | Train Loss: 4.3130 | Val Loss: 1.7565 | Val Acc: 0.3214
2025-06-22 01:57:03,414 [INFO] Epoch 008 | Train Loss: 3.7777 | Val Loss: 1.8509 | Val Acc: 0.4286
2025-06-22 01:57:03,430 [INFO] Epoch 009 | Train Loss: 3.5355 | Val Loss: 1.7985 | Val Acc: 0.4107
2025-06-22 01:57:03,447 [INFO] Epoch 010 | Train Loss: 3.2497 | Val Loss: 1.8229 | Val Acc: 0.3750
2025-06-22 01:57:03,992 [INFO] Epoch 011 | Train Loss: 3.6247 | Val Loss: 1.7787 | Val Acc: 0.3393
2025-06-22 01:57:04,010 [INFO] Epoch 012 | Train Loss: 3.1515 | Val Loss: 1.8061 | Val Acc: 0.3036
2025-06-22 01:57:04,029 [INFO] Epoch 013 | Train Loss: 3.3189 | Val Loss: 1.7911 | Val Acc: 0.3929
2025-06-22 01:57:04,046 [INFO] Epoch 014 | Train Loss: 2.6502 | Val Loss: 1.8014 | Val Acc: 0.3750
2025-06-22 01:57:04,064 [INFO] Epoch 015 | Train Loss: 2.5057 | Val Loss: 1.6928 | Val Acc: 0.3750
2025-06-22 01:57:04,082 [INFO] Epoch 016 | Train Loss: 2.7630 | Val Loss: 1.6574 | Val Acc: 0.3214
2025-06-22 01:57:04,106 [INFO] Epoch 017 | Train Loss: 2.9386 | Val Loss: 1.5607 | Val Acc: 0.2857
2025-06-22 01:57:04,128 [INFO] Epoch 018 | Train Loss: 2.4582 | Val Loss: 1.5749 | Val Acc: 0.3750
2025-06-22 01:57:04,148 [INFO] Epoch 019 | Train Loss: 2.9708 | Val Loss: 1.6065 | Val Acc: 0.3750
2025-06-22 01:57:04,166 [INFO] Epoch 020 | Train Loss: 2.4950 | Val Loss: 1.6858 | Val Acc: 0.3393
2025-06-22 01:57:04,849 [INFO] Epoch 021 | Train Loss: 2.4560 | Val Loss: 1.9760 | Val Acc: 0.3214
2025-06-22 01:57:04,870 [INFO] Epoch 022 | Train Loss: 2.4215 | Val Loss: 1.8574 | Val Acc: 0.2679
2025-06-22 01:57:04,890 [INFO] Epoch 023 | Train Loss: 2.5672 | Val Loss: 1.7308 | Val Acc: 0.2679
2025-06-22 01:57:04,913 [INFO] Epoch 024 | Train Loss: 2.1823 | Val Loss: 1.5370 | Val Acc: 0.3214
2025-06-22 01:57:04,935 [INFO] Epoch 025 | Train Loss: 2.3775 | Val Loss: 1.5837 | Val Acc: 0.3214
2025-06-22 01:57:04,958 [INFO] Epoch 026 | Train Loss: 2.2403 | Val Loss: 1.5753 | Val Acc: 0.3214
2025-06-22 01:57:04,981 [INFO] Epoch 027 | Train Loss: 2.6730 | Val Loss: 1.5625 | Val Acc: 0.3571
2025-06-22 01:57:05,009 [INFO] Epoch 028 | Train Loss: 1.9038 | Val Loss: 1.4989 | Val Acc: 0.3571
2025-06-22 01:57:05,032 [INFO] Epoch 029 | Train Loss: 2.3485 | Val Loss: 1.4989 | Val Acc: 0.4286
2025-06-22 01:57:05,052 [INFO] Epoch 030 | Train Loss: 2.5097 | Val Loss: 1.7123 | Val Acc: 0.3393
2025-06-22 01:57:05,739 [INFO] Epoch 031 | Train Loss: 1.9828 | Val Loss: 1.5655 | Val Acc: 0.3036
2025-06-22 01:57:05,760 [INFO] Epoch 032 | Train Loss: 1.9591 | Val Loss: 1.5395 | Val Acc: 0.3571
2025-06-22 01:57:05,780 [INFO] Epoch 033 | Train Loss: 1.7905 | Val Loss: 1.6969 | Val Acc: 0.3214
2025-06-22 01:57:05,806 [INFO] Epoch 034 | Train Loss: 1.7845 | Val Loss: 1.6541 | Val Acc: 0.2321
2025-06-22 01:57:05,826 [INFO] Epoch 035 | Train Loss: 1.9069 | Val Loss: 1.5522 | Val Acc: 0.3571
2025-06-22 01:57:05,846 [INFO] Epoch 036 | Train Loss: 2.1295 | Val Loss: 1.6613 | Val Acc: 0.3571
2025-06-22 01:57:05,866 [INFO] Epoch 037 | Train Loss: 2.0759 | Val Loss: 1.6664 | Val Acc: 0.3750
2025-06-22 01:57:05,885 [INFO] Epoch 038 | Train Loss: 1.6064 | Val Loss: 1.6953 | Val Acc: 0.3393
2025-06-22 01:57:05,904 [INFO] Epoch 039 | Train Loss: 1.6991 | Val Loss: 1.6308 | Val Acc: 0.3214
2025-06-22 01:57:05,923 [INFO] Epoch 040 | Train Loss: 1.9058 | Val Loss: 1.5775 | Val Acc: 0.3393
2025-06-22 01:57:06,595 [INFO] Epoch 041 | Train Loss: 1.6594 | Val Loss: 1.4855 | Val Acc: 0.3393
2025-06-22 01:57:06,614 [INFO] Epoch 042 | Train Loss: 1.4161 | Val Loss: 1.5154 | Val Acc: 0.3571
2025-06-22 01:57:06,632 [INFO] Epoch 043 | Train Loss: 2.3115 | Val Loss: 1.4755 | Val Acc: 0.3214
2025-06-22 01:57:06,649 [INFO] Epoch 044 | Train Loss: 1.8141 | Val Loss: 1.4966 | Val Acc: 0.3571
2025-06-22 01:57:06,673 [INFO] Epoch 045 | Train Loss: 1.3618 | Val Loss: 1.5147 | Val Acc: 0.3929
2025-06-22 01:57:06,692 [INFO] Epoch 046 | Train Loss: 1.5336 | Val Loss: 1.6759 | Val Acc: 0.3929
2025-06-22 01:57:06,712 [INFO] Epoch 047 | Train Loss: 1.8017 | Val Loss: 1.6810 | Val Acc: 0.3929
2025-06-22 01:57:06,731 [INFO] Epoch 048 | Train Loss: 1.6901 | Val Loss: 1.5611 | Val Acc: 0.4286
2025-06-22 01:57:06,750 [INFO] Epoch 049 | Train Loss: 1.6671 | Val Loss: 1.5862 | Val Acc: 0.3214
2025-06-22 01:57:06,770 [INFO] Epoch 050 | Train Loss: 1.6940 | Val Loss: 1.6575 | Val Acc: 0.3750
2025-06-22 01:57:07,483 [INFO] Best model saved to <built-in function time>/ENZYMES/best/best_source_gin.pth (Val Acc: 0.4286 at epoch 3)
2025-06-22 01:57:07,484 [INFO] Training/validation metrics saved to <built-in function time>/ENZYMES/GIN_0.001/train_metrics.csv
2025-06-22 01:57:08,020 [INFO] Final training curves saved
2025-06-22 01:57:08,020 [INFO] Training completed!
2025-06-22 01:57:08,020 [INFO] Best validation accuracy: 0.4286
2025-06-22 01:57:08,020 [INFO] Best epoch: 3
2025-06-22 01:57:08,020 [INFO] Total epochs trained: 50
2025-06-22 01:57:08,021 [INFO] Starting training with parameters: (0.001, 0.2, 32, 2, 50, 0.001)
